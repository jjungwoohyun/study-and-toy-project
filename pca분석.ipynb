{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**차원 축소**\n",
    "\n",
    "* 불필요한 차원을 제거하기 위함 (차원 축소)\n",
    "    - 변수 선택(selection) : 분석 목적에 부합하는 소수의 예측변수만 선택\n",
    "        - 선택한 변수 해석이 용이하지만 변수간 상관관계를 고려하기 어려움\n",
    "    \n",
    "    - 변수추출(extraction) : 예측변수의 변환을 통해 새로운 변수 추출\n",
    "        - 변수간 상관관계를 고려, 일반적으로 변수의 갯수를 많이 줄일 수 있지만 추출된 변수의 해석이 어렵다.\n",
    "\n",
    "    - 지도학습과 비지도학습으로 두 가지로 분류\n",
    "\n",
    "    - Supervised feature selection : Y를 이용해 주요 변수를 선택 하겠다.\n",
    "        - Stepwise regression, LASSO etc..\n",
    "    - Supervised feature exraction : Partial least squares(PLS)\n",
    "    - Unsupervised feature selection : PCA loading\n",
    "    - Unsupervised featrue extraction : **PCA**, Wavelets transforms, Autoencoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA(Principal Components Analysis)**\n",
    "* 고차원 데이터를 효과적으로 분석하기 위한 대표 분석 기법\n",
    "* 차원축소, 시각화, 군집화, 압축\n",
    "\n",
    "* PCA는 n개의 관측치와 p개의 변수로 구성된 데이터를 상관관계가 없는 k개의 변수로 구성된 데이터로 요약하는 방식으로, 요약된 변수는 기존 변수의 Linear Combination으로 생성\n",
    "* 원래 데이터의 분산을 최대한 보존하는 새로운 축을 찾고, 그 축에 데이터를 Projection 시키는 기법\n",
    "\n",
    "* 데이터 차원 축소 (n by p -> n by k where k << p)\n",
    "* 일반적으로 분석 초기 단계에서 사용\n",
    "\n",
    "* Z is a linear combination of the original p variables in X\n",
    "    - original variable들의 i번째 기저(basis) 또는 계수(Loading) 다르게해서 더하거나 뺀다\n",
    "    - 각 기저로 Projection 변환 후 변수 (주성분 Z)\n",
    "\n",
    "*  X 변수의 Mean Vetor, Covariance Matrix (P개 변수에 대한 분산, 두개 변수의 공분산), correlation Matrix(두 변수의 상관계수)\n",
    "\n",
    "* Projection ? \n",
    "    - vector b로부터 vector a에 수직인 점까지의 길이를 가지며 vector a와 같은 방향을 갖는 벡터 vector x = p(수직인 점까지의 길이) * vector a\n",
    "\n",
    "* Eigenvalue & Eigenvector ? \n",
    "    - 어떤 행렬 A에 대해 상수 람다와 벡터 x가 다음 식을 만족할 때, 람다와 x를 각각 행렬 A의 고유값(eigenvalue), 고유벡터(eigenvector)라고 한다.\n",
    "    - Ax = lambda x -> (A-lambda I)x = 0\n",
    "\n",
    "    - Linear Transformation A에 의한 변환 결과가 자기 자신의 상수배가 되는 0이 아닌 벡터를 고유벡터라 하고, 이 상수배 값을 고유값이라 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Z를 찾는 방법 ?**\n",
    "\n",
    "* Assume that we have the centered data (모든 변수의 평균은 0)\n",
    "* Let X be an p-dimensional ramdom vector(x1 ~ xp의 고차원 데이터) with the covariance matrix sigma(p * p matrix)\n",
    "* Let alpha be an p-dimensional vetor of length one\n",
    "* Let Z = alpha transpose * X be the projection of X onto the direction alpha\n",
    "\n",
    "**to find alpha the produces the largest variance of Z**\n",
    "* Max Var(Z) = Var(a^TX) = a^TVar(X)a = a^T sigma alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "513dc2e41d739bb2c947903f3c0bbf636d03aa53ab50e61c694a27481c81805e"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
